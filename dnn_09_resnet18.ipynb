{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FRya3-1t7gO7"
      },
      "source": [
        "# Wstęp\n",
        "\n",
        "Metody uczenia maszynowego możemy podzielić na dwie główne kategorie (pomijając uczenie ze wzmocnieniem): nadzorowane i nienadzorowane. Uczenie **nadzorowane** (ang. *supervised*) to jest uczenie z dostępnymi etykietami dla danych wejściowych. Na parach danych uczących $dataset= \\{(x_0,y_0), (x_1,y_1), \\ldots, (x_n,y_n)\\}$ model ma za zadanie nauczyć się funkcji $f: X \\rightarrow Y$. Z kolei modele uczone w sposób **nienadzorowany** (ang. *unsupervised*) wykorzystują podczas trenowania dane nieetykietowane tzn. nie znamy $y$ z pary $(x, y)$.\n",
        "\n",
        "Dość częstą sytuacją, z jaką mamy do czynienia, jest posiadanie małego podziobioru danych etykietowanych i dużego nieetykietowanych. Często annotacja danych wymaga ingerencji człowieka - ktoś musi określić co jest na obrazku, ktoś musi powiedzieć czy dane słowo jest rzeczownkiem czy czasownikiem itd.\n",
        "\n",
        "Jeżeli mamy dane etykietowane do zadania uczenia nadzorowanego (np. klasyfikacja obrazka), ale także dużą ilość danych nieetykietowanych, to możemy wtedy zastosować techniki **uczenia częściowo nadzorowanego** (ang. *semi-supervised learning*). Te techniki najczęściej uczą się funkcji $f: X \\rightarrow Y$, ale jednocześnie są w stanie wykorzystać informacje z danych nieetykietowanych do poprawienia działania modelu."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjjlvGdZNg00"
      },
      "source": [
        "## Cel ćwiczenia\n",
        "\n",
        "Celem ćwiczenia jest nauczenie modelu z wykorzystaniem danych etykietowanych i nieetykietowanych ze zbioru STL10 z użyciem metody [Bootstrap your own latent](https://arxiv.org/abs/2006.07733).\n",
        "\n",
        "Metoda ta jest relatywnie \"lekka\" obliczeniowo, a także dość prosta do zrozumienia i zaimplementowania, dlatego też na niej się skupimy na tych laboratoriach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uI8ZMEH2NkgA"
      },
      "source": [
        "# Zbiór STL10\n",
        "\n",
        "Zbiór STL10 to zbiór stworzony i udostępniony przez Stanford [[strona]](https://ai.stanford.edu/~acoates/stl10/) [[papier]](https://cs.stanford.edu/~acoates/papers/coatesleeng_aistats_2011.pdf) a inspirowany przez CIFAR-10. Obrazy zostały pozyskane z [ImageNet](https://image-net.org/). Szczegóły można doczytać na ich stronie. To co jest ważne to to, że autorzy zbioru dostarczają predefiniowany plan eksperymentalny, żeby móc porównywać łatwo wyniki eksperymentów. Nie będziemy go tutaj stosować z uwagi na jego czasochłonność (10 foldów), ale warto pamiętać o tym, że często są z góry ustalone sposoby walidacji zaprojetowanych przez nas algorytmów na określonych zbiorach referencyjnych.\n",
        "\n",
        "Korzystając z `torchvision.datasets` ***załaduj*** 3 podziały zbioru danych STL10: `train`, `test`, `unlabeled` oraz utwórz z nich instancje klasy `DataLoader`. Korzystając z Google Colab rozważ użycie Google Drive do przechowyania zbioru w calu zaoszczędzenia czasu na wielokrotne pobieranie."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hC8VhuEoR90S",
        "outputId": "090c3df8-592d-4ca3-f01d-7ced5d5f4250"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torchvision.datasets as datasets\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import os\n",
        "from google.colab import drive\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "path = '/content/drive/MyDrive/lab09/data/'\n",
        "os.makedirs(path, exist_ok=True)\n",
        "\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n",
        "\n",
        "\n",
        "train_data = datasets.STL10(path, split='train', download=True, transform=transform)\n",
        "test_data = datasets.STL10(path, split='test', download=True, transform=transform)\n",
        "unlabeled_data = datasets.STL10(path, split='unlabeled', download=True, transform=transform)\n",
        "\n",
        "# dataset_length = len(unlabeled_data)\n",
        "# train_length = int(0.70 * dataset_length)\n",
        "# val_length = dataset_length - train_length\n",
        "\n",
        "# # Split the dataset\n",
        "# unlabeled_data, val_dataset = torch.utils.data.random_split(unlabeled_data, [train_length, val_length])\n",
        "\n",
        "\n",
        "\n",
        "batch_size = 32  # Możesz dostosować rozmiar partii do swoich potrzeb\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
        "unlabeled_loader = DataLoader(unlabeled_data, batch_size=256, shuffle=True)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4qyXdlLZHzn"
      },
      "source": [
        "# Uczenie nadzorowane\n",
        "\n",
        "Żeby porównać czy metoda BYOL przynosi nam jakieś korzyści musimy wyznaczyć wartość bazową metryk(i) jakości, których będziemu używać (np. dokładność).\n",
        "\n",
        "***Zaimplementuj*** wybraną metodę uczenia nadzorowanego na danych `train` z STL10. Możesz wykorzystać predefiniowane architektury w `torchvision.models` oraz kody źródłowe z poprzednich list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l2vcmEhEaA2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c35dad2-d6dd-447f-edc6-dfd6f3c6d327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "class SimpleCNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, repr=80):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),  # Assuming input image size is 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, repr)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class MyLinear(torch.nn.Module):\n",
        "  def __init__(self, encoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(128, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10),\n",
        "\n",
        "    )\n",
        "\n",
        "    # for param in self.encoder.parameters():\n",
        "    #     param.requires_grad = False\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fhPW9YzQ04za"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import Tuple\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def count_correct(\n",
        "    y_pred: torch.Tensor, y_true: torch.Tensor\n",
        ") -> torch.Tensor:\n",
        "    preds = torch.argmax(y_pred, dim=1)\n",
        "    return (preds == y_true).float().sum()\n",
        "\n",
        "def validate(\n",
        "    model: nn.Module,\n",
        "    loss_fn: torch.nn.CrossEntropyLoss,\n",
        "    dataloader: DataLoader\n",
        ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    loss = 0\n",
        "    correct = 0\n",
        "    all = 0\n",
        "    for X_batch, y_batch in dataloader:\n",
        "        y_pred = model(X_batch.cuda())\n",
        "        all += len(y_pred)\n",
        "        loss += loss_fn(y_pred, y_batch.cuda()).sum()\n",
        "        correct += count_correct(y_pred, y_batch.cuda())\n",
        "    return loss / all, correct / all\n",
        "\n",
        "def fit(\n",
        "    model: nn.Module, optimiser: optim.Optimizer,\n",
        "    loss_fn: torch.nn.CrossEntropyLoss,\n",
        "    train_dl: DataLoader,\n",
        "    val_dl: DataLoader,\n",
        "    epochs: int,\n",
        "    print_metrics: bool = False,\n",
        "    name: str ='',\n",
        "    patience: int = 3,\n",
        "):\n",
        "    last_better_epoch = 0\n",
        "    low_val_loss = float('inf')\n",
        "    low_acc_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in tqdm(train_dl, desc=f\"Epoch {epoch}\"):\n",
        "            X_batch = X_batch.to('cuda')\n",
        "            y_batch = y_batch.to('cuda')\n",
        "            y_pred = model(X_batch)\n",
        "            loss = loss_fn(y_pred, y_batch.long())\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_loss, train_acc = validate(model, loss_fn, train_dl)\n",
        "            val_loss, val_acc = validate(model, loss_fn, val_dl)\n",
        "\n",
        "            if val_loss < low_val_loss:\n",
        "                low_val_loss = val_loss\n",
        "                low_acc_loss = val_acc\n",
        "                last_better_epoch = epoch\n",
        "\n",
        "            if patience is not None and epoch - last_better_epoch > patience:\n",
        "                print(f\"Early stopping on epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "            # writer.add_scalars(\n",
        "            #     main_tag='loss',\n",
        "            #     tag_scalar_dict={\n",
        "            #         f'train_{name}': train_loss,\n",
        "            #         f'dev_{name}': val_loss\n",
        "            #     },\n",
        "            #     global_step=epoch+1\n",
        "            # )\n",
        "\n",
        "            # writer.add_scalars(\n",
        "            #     main_tag=f'acc',\n",
        "            #     tag_scalar_dict={\n",
        "            #         f'train_{name}': train_acc,\n",
        "            #         f'dev_{name}': val_acc\n",
        "            #     },\n",
        "            #     global_step=epoch+1\n",
        "            # )\n",
        "            if print_metrics:\n",
        "                print(\n",
        "                    f\"Epoch {name} {epoch}: \"\n",
        "                    f\"train loss = {train_loss:.3f} (acc: {train_acc:.3f}), \"\n",
        "                    f\"validation loss = {val_loss:.3f} (acc: {val_acc:.3f})\"\n",
        "                )\n",
        "\n",
        "    return low_val_loss, low_acc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU2MGUJd04za",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55759fbc-4a36-4a26-feef-8087e90edb75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 157/157 [00:05<00:00, 28.05it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch vgg19 0: train loss = 0.033 (acc: 0.618), validation loss = 0.039 (acc: 0.565)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 157/157 [00:04<00:00, 32.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch vgg19 1: train loss = 0.021 (acc: 0.792), validation loss = 0.028 (acc: 0.703)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 157/157 [00:04<00:00, 32.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch vgg19 2: train loss = 0.019 (acc: 0.805), validation loss = 0.030 (acc: 0.688)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 157/157 [00:05<00:00, 27.45it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch vgg19 3: train loss = 0.017 (acc: 0.836), validation loss = 0.034 (acc: 0.690)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 157/157 [00:05<00:00, 28.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch vgg19 4: train loss = 0.012 (acc: 0.890), validation loss = 0.029 (acc: 0.718)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 157/157 [00:04<00:00, 33.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping on epoch 5\n",
            "(tensor(0.0281, device='cuda:0'), tensor(0.7034, device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "model = MyLinear(SimpleCNNEncoder(repr=128)).cuda()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "result = fit(model, optimiser, loss_fn, train_loader, test_loader, 50, print_metrics=True, name='vgg19')\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saaKpwl0FVII"
      },
      "source": [
        "# Bootstrap your own latent\n",
        "\n",
        "Metoda [Bootstrap your own latent](https://arxiv.org/abs/2006.07733) jest opisana w rodziale 3.1 papieru a także w dodatku A. Składa się z dwóch etapów:\n",
        "\n",
        "\n",
        "1.   uczenia samonadzorowanego (ang. *self-supervised*)\n",
        "2.   douczania nadzorowanego (ang. *fine-tuning*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7b8L_zYGNs_K"
      },
      "source": [
        "## Uczenie samonadzorowane\n",
        "\n",
        "Architektura do nauczania samonadzorowanego składa się z dwóch sieci: (1) *online* i (2) *target*. W uproszczeniu cała architektura działa tak:\n",
        "\n",
        "\n",
        "1.   Dla obrazka $x$ wygeneruj dwie różne augmentacje $v$ i $v'$ za pomocą funkcji $t$ i $t'$.\n",
        "2.   Widok $v$ przekazujemy do sieci *online*, a $v'$ do *target*.\n",
        "3.   Następnie widoki przekształacamy za pomocą sieci do uczenia reprezentacji (np. resnet18 lub resnet50) do reprezentacji $y_\\theta$ i $y'_\\xi$.\n",
        "4.   Potem dokonujemy projekcji tych reprezentacji w celu zmniejszenia wymiarowości (np. za pomocą sieci MLP).\n",
        "5.   Na sieci online dokonujmey dodatkowo predykcji pseudo-etykiety (ang. *pseudolabel*)\n",
        "6.   Wyliczamy fukncję kosztu: MSE z wyjścia predyktora sieci *online* oraz wyjścia projekcji sieci *target* \"przepuszczonej\" przez predyktor sieci *online* **bez propagacji wstecznej** (*vide Algorithm 1* z papieru).\n",
        "7.   Dokonujemy wstecznej propagacji **tylko** po sieci *online*.\n",
        "8.   Aktualizujemy wagi sieci *target* sumując w ważony sposób wagi obu sieci $\\xi = \\tau\\xi + (1 - \\tau)\\theta$ ($\\tau$ jest hiperprametrem) - jest to ruchoma średnia wykładnicza (ang. *moving exponential average*).\n",
        "\n",
        "Po zakończeniu procesu uczenia samonadzorowanego zostawiamy do douczania sieć kodera *online* $f_\\theta$. Cała sieć *target* oraz warstwy do projekcji i predykcji w sieci *online* są \"do wyrzucenia\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFIRw--bTYe5"
      },
      "source": [
        "### Augmentacja\n",
        "\n",
        "Dodatek B publikacji opisuje augmentacje zastosowane w metodzie BYOL. Zwróć uwagę na tabelę 6 w publikacji. `torchvision.transforms.RandomApply` może być pomocne.\n",
        "\n",
        "***Zaimeplementuj*** augmentację $\\tau$ i $\\tau'$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "csbR-Bvy8IbZ"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from torch import nn\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "\n",
        "\n",
        "def get_default_aug() -> nn.Module:\n",
        "    return torch.nn.Sequential(\n",
        "        RandomApply(T.RandomRotation(degrees=(10, 60)), p=0.2).cuda(),\n",
        "        T.RandomHorizontalFlip(),\n",
        "        RandomApply(T.GaussianBlur((3, 3), (1.0, 2.0)), p=0.2).cuda(),\n",
        "    )\n",
        "\n",
        "\n",
        "class RandomApply(nn.Module):\n",
        "    def __init__(self, fn: nn.Module, p: float):\n",
        "        super().__init__()\n",
        "        self.fn = fn\n",
        "        self.p = p\n",
        "\n",
        "    def forward(self, x):\n",
        "        if random.random() > self.p:\n",
        "            return x\n",
        "        return self.fn(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FKMQGx8FtoF"
      },
      "source": [
        "### Implementacja uczenia samonadzorowanego\n",
        "\n",
        "***Zaprogramuj*** proces uczenia samonadzorowanego na danych nieetykietowanych ze zbioru STL10.\n",
        "\n",
        "Wskazówki do realizacji polecenia:\n",
        "\n",
        "1. Proces uczenia może trwać bardzo długo dlatego zaleca się zastsowanie wczesnego zatrzymania lub uczenia przez tylko jedną epokę. Mimo wszystko powinno się dać osiągnąć poprawę w uczeniu nadzorowanym wykorzystując tylko zasoby z Google Colab.\n",
        "2. Dobrze jest pominąć walidację na zbiorze treningowym i robić ją tylko na zbiorze walidacyjnym - zbiór treningowy jest ogromny i w związku z tym narzut czasowy na walidację też będzie duży.\n",
        "3. Walidację modelu można przeprowadzić na zbiorze `train` lub całkowicie ją pominąć, jeżeli uczymy na stałej ilości epok.\n",
        "4. Rozważ zastosowanie tylko jednej augmentacji - augmentacja $\\tau'$ jest bardziej czasochłonna niż $\\tau$.\n",
        "5. Poniżej jest zaprezentowany zalążek kodu - jest on jedynie wskazówką i można na swój sposób zaimplementować tę metodę"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "s5NarrwBiJIk",
        "outputId": "65b08d7b-7ada-4d5c-c373-c145b75a2cc8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import copy\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "\n",
        "resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class SimpleCNNEncoder(torch.nn.Module):\n",
        "    def __init__(self, repr=80):\n",
        "        super().__init__()\n",
        "        self.model = nn.Sequential(*list(resnet.children())[:-1])\n",
        "\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(512, 256),  # Assuming input image size is 32x32\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(256, repr)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "class BYOL(nn.Module):\n",
        "    def __init__(self, projection_size=256, labels_no=1000, tau=0.999):\n",
        "        super().__init__()\n",
        "        repr = 128\n",
        "        self.online_encoder = SimpleCNNEncoder(repr=repr)\n",
        "        self.online_projector = self.mlp(repr, projection_size)\n",
        "        self.online_predictor = self.mlp(projection_size, labels_no)\n",
        "        self.online_net = nn.Sequential(\n",
        "            self.online_encoder,\n",
        "            self.online_projector,\n",
        "            self.online_predictor\n",
        "        )\n",
        "\n",
        "        self.target_encoder = self.copy_and_freeze_module(self.online_encoder)\n",
        "        self.target_projector = self.copy_and_freeze_module(self.mlp(repr, labels_no))\n",
        "\n",
        "        self.target_net = nn.Sequential(self.target_encoder, self.target_projector)\n",
        "        for param in self.target_net.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.aug_1 = get_default_aug()\n",
        "        self.aug_2 = get_default_aug()\n",
        "\n",
        "        self.tau = tau\n",
        "\n",
        "    def forward(self, x: Tensor) -> tuple[Tensor, Tensor]:\n",
        "        t = self.aug_1(x)\n",
        "        t_prim = self.aug_2(x)\n",
        "\n",
        "        q = self.online_net(t)\n",
        "        q_sym = self.online_net(t_prim)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            z_prim = self.target_net(t_prim)\n",
        "            z_prim_sym = self.target_net(t)\n",
        "\n",
        "        q = torch.cat([q, q_sym], dim=0)\n",
        "        z_prim = torch.cat([z_prim, z_prim_sym], dim=0)\n",
        "\n",
        "        return q, z_prim\n",
        "\n",
        "\n",
        "    def mlp(self, encoder_out_shape, projection_size):\n",
        "      return nn.Sequential(\n",
        "          nn.Linear(encoder_out_shape, projection_size),\n",
        "          nn.ReLU(),\n",
        "          nn.BatchNorm1d(projection_size),\n",
        "          nn.Linear(projection_size, projection_size)\n",
        "      )\n",
        "\n",
        "    def byol_loss(self, q: Tensor, z_prim: Tensor) -> Tensor:\n",
        "        q_norm = F.normalize(q, dim=-1, p=2)\n",
        "        z_prim_norm = F.normalize(z_prim, dim=-1, p=2)\n",
        "\n",
        "        return( 2 - 2 * (q_norm * z_prim_norm).sum(dim=-1)).mean()\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def update_target_network(self):\n",
        "        for online_params, target_params in zip(self.online_net[0].parameters(), self.target_net[0].parameters()):\n",
        "            target_params.data = target_params.data * self.tau + online_params.data * (1 - self.tau)\n",
        "\n",
        "\n",
        "    def copy_and_freeze_module(self, model: nn.Module) -> nn.Module:\n",
        "        copy_of_model = copy.deepcopy(model)\n",
        "        for param in copy_of_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        return copy_of_model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "qqdlRax59q0K"
      },
      "outputs": [],
      "source": [
        "def fit_ssl(\n",
        "    model: nn.Module, optimiser: optim.Optimizer,\n",
        "    train_dl: DataLoader,\n",
        "    epochs: int,\n",
        "    print_metrics: bool = False,\n",
        "    name: str ='',\n",
        "):\n",
        "    last_better_epoch = 0\n",
        "    low_val_loss = float('inf')\n",
        "    low_acc_loss = float('inf')\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        losses = []\n",
        "\n",
        "        for X_batch, y_batch in tqdm(train_dl, desc=f\"Epoch {epoch}\"):\n",
        "            X_batch = X_batch.to('cuda')\n",
        "            y_batch = y_batch.to('cuda')\n",
        "            q, z = model(X_batch)\n",
        "            loss = model.byol_loss(q, z)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            loss.backward()\n",
        "            optimiser.step()\n",
        "            optimiser.zero_grad()\n",
        "\n",
        "        model.update_target_network()\n",
        "        model.eval()\n",
        "        loss = sum(losses) / len(losses)\n",
        "\n",
        "        if print_metrics:\n",
        "            print(\n",
        "                f\"Epoch {name} {epoch}: \"\n",
        "                f\"train loss = {loss} \"\n",
        "            )\n",
        "\n",
        "    return low_val_loss, low_acc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAdznk5P993j",
        "outputId": "b4e59329-d937-4a24-d1e9-2fc6ffbc5807"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 391/391 [02:46<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch byol 0: train loss = 1.5793217390089693 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 391/391 [02:44<00:00,  2.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch byol 1: train loss = 1.5113740650284322 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 391/391 [02:45<00:00,  2.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch byol 2: train loss = 1.482897459393572 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 391/391 [02:44<00:00,  2.37it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch byol 3: train loss = 1.4636101558080414 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "model_ssl = BYOL().cuda()\n",
        "optimiser = optim.Adam(model_ssl.parameters(), lr=0.001)\n",
        "\n",
        "result = fit_ssl(model_ssl, optimiser, unlabeled_loader, 4, print_metrics=True, name='byol')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qitno0wc8W35"
      },
      "source": [
        "## Douczanie nadzorowane\n",
        "\n",
        "***Zaimplementuj*** proces douczania kodera z poprzedniego polecenia na danych etykietowanych ze zbioru treningowego. Porównaj jakość tego modelu z modelem nauczonym tylko na danych etykietownaych. Postaraj się wyjaśnić różnice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "as-cAUfFJiYL"
      },
      "outputs": [],
      "source": [
        "state_dict = model_ssl.online_encoder.state_dict()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BiMQc6aZXgI_"
      },
      "outputs": [],
      "source": [
        "encoder = SimpleCNNEncoder(repr=128)\n",
        "encoder.load_state_dict(state_dict)\n",
        "\n",
        "class MySSLinear(torch.nn.Module):\n",
        "  def __init__(self, encoder):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder\n",
        "\n",
        "    self.fc = nn.Sequential(\n",
        "        nn.Linear(128, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128, 128),\n",
        "        nn.BatchNorm1d(128),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(128, 10),\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.fc(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "UANjb5voBaFO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e23bd52d-b188-4ea4-ae3a-747205973979"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 157/157 [00:06<00:00, 24.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 0: train loss = 0.032 (acc: 0.671), validation loss = 0.036 (acc: 0.626)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 157/157 [00:05<00:00, 26.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 1: train loss = 0.018 (acc: 0.813), validation loss = 0.027 (acc: 0.710)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 157/157 [00:05<00:00, 30.78it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 2: train loss = 0.014 (acc: 0.853), validation loss = 0.027 (acc: 0.721)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 157/157 [00:06<00:00, 25.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 3: train loss = 0.010 (acc: 0.897), validation loss = 0.025 (acc: 0.743)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 157/157 [00:05<00:00, 27.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 4: train loss = 0.006 (acc: 0.942), validation loss = 0.024 (acc: 0.772)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 157/157 [00:05<00:00, 30.17it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 5: train loss = 0.007 (acc: 0.929), validation loss = 0.028 (acc: 0.740)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 157/157 [00:05<00:00, 29.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 6: train loss = 0.004 (acc: 0.959), validation loss = 0.025 (acc: 0.772)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 157/157 [00:06<00:00, 25.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 7: train loss = 0.006 (acc: 0.947), validation loss = 0.033 (acc: 0.743)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 157/157 [00:05<00:00, 31.08it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping on epoch 8\n",
            "(tensor(0.0239, device='cuda:0'), tensor(0.7718, device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "model = MySSLinear(encoder).cuda()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "result = fit(model, optimiser, loss_fn, train_loader, test_loader, 50, print_metrics=True, name='ft', patience=3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VJB6wW0CLB9E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5e79b8c-900a-4a0c-9e04-4902a1f41d0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 157/157 [00:05<00:00, 30.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 0: train loss = 0.036 (acc: 0.995), validation loss = 0.046 (acc: 0.809)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 157/157 [00:06<00:00, 25.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 1: train loss = 0.031 (acc: 0.999), validation loss = 0.042 (acc: 0.818)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 157/157 [00:05<00:00, 28.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 2: train loss = 0.031 (acc: 1.000), validation loss = 0.042 (acc: 0.823)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 157/157 [00:05<00:00, 30.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 3: train loss = 0.030 (acc: 1.000), validation loss = 0.041 (acc: 0.823)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 157/157 [00:05<00:00, 27.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 4: train loss = 0.029 (acc: 1.000), validation loss = 0.040 (acc: 0.826)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 157/157 [00:06<00:00, 25.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 5: train loss = 0.030 (acc: 1.000), validation loss = 0.040 (acc: 0.824)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 157/157 [00:05<00:00, 30.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 6: train loss = 0.028 (acc: 1.000), validation loss = 0.039 (acc: 0.829)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 157/157 [00:05<00:00, 30.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 7: train loss = 0.029 (acc: 1.000), validation loss = 0.040 (acc: 0.823)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 157/157 [00:06<00:00, 24.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 8: train loss = 0.028 (acc: 1.000), validation loss = 0.039 (acc: 0.828)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 157/157 [00:05<00:00, 30.88it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 9: train loss = 0.028 (acc: 1.000), validation loss = 0.039 (acc: 0.828)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 157/157 [00:05<00:00, 26.49it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 10: train loss = 0.028 (acc: 0.999), validation loss = 0.039 (acc: 0.826)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 157/157 [00:06<00:00, 25.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 11: train loss = 0.028 (acc: 0.999), validation loss = 0.039 (acc: 0.822)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 157/157 [00:05<00:00, 30.03it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 12: train loss = 0.028 (acc: 0.999), validation loss = 0.040 (acc: 0.816)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 157/157 [00:05<00:00, 31.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch ft 13: train loss = 0.028 (acc: 0.999), validation loss = 0.039 (acc: 0.821)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 157/157 [00:05<00:00, 26.22it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Early stopping on epoch 14\n",
            "(tensor(0.0388, device='cuda:0'), tensor(0.8255, device='cuda:0'))\n"
          ]
        }
      ],
      "source": [
        "model = MySSLinear(encoder).cuda()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "result = fit(model, optimiser, loss_fn, train_loader, test_loader, 50, print_metrics=True, name='ft', patience=3)\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "OOcoaUZl_xeV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}